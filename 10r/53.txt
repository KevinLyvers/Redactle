https://en.wikipedia.org/wiki/Parallel_computing
parallel, computing, is, a, type, of, computation, in, which, many, calculations, or, processes, are, carried, out, simultaneously, large, problems, can, often, be, divided, into, smaller, ones, which, can, then, be, solved, at, the, same, time, there, are, several, different, forms, of, parallel, computing, bit, level, instruction, level, data, and, task, parallelism, parallelism, has, long, been, employed, in, high, performance, computing, but, has, gained, broader, interest, due, to, the, physical, constraints, preventing, frequency, scaling, as, power, consumption, and, consequently, heat, generation, by, computers, has, become, a, concern, in, recent, years, parallel, computing, has, become, the, dominant, paradigm, in, computer, architecture, mainly, in, the, form, of, multi, core, processors, in, computer, science, parallelism, and, concurrency, are, two, different, things, a, parallel, program, uses, multiple, cpu, cores, each, core, performing, a, task, independently, on, the, other, hand, concurrency, enables, a, program, to, deal, with, multiple, tasks, even, on, a, single, cpu, core, the, core, switches, between, tasks, i, e, threads, without, necessarily, completing, each, one, a, program, can, have, both, neither, or, a, combination, of, parallelism, and, concurrency, characteristics, parallel, computers, can, be, roughly, classified, according, to, the, level, at, which, the, hardware, supports, parallelism, with, multi, core, and, multi, processor, computers, having, multiple, processing, elements, within, a, single, machine, while, clusters, mpps, and, grids, use, multiple, computers, to, work, on, the, same, task, specialized, parallel, computer, architectures, are, sometimes, used, alongside, traditional, processors, for, accelerating, specific, tasks, in, some, cases, parallelism, is, transparent, to, the, programmer, such, as, in, bit, level, or, instruction, level, parallelism, but, explicitly, parallel, algorithms, particularly, those, that, use, concurrency, are, more, difficult, to, write, than, sequential, ones, because, concurrency, introduces, several, new, classes, of, potential, software, bugs, of, which, race, conditions, are, the, most, common, communication, and, synchronization, between, the, different, subtasks, are, typically, some, of, the, greatest, obstacles, to, getting, optimal, parallel, program, performance, a, theoretical, upper, bound, on, the, speed, up, of, a, single, program, as, a, result, of, parallelization, is, given, by, amdahl, s, law, which, states, that, it, is, limited, by, the, fraction, of, time, for, which, the, parallelization, can, be, utilised, traditionally, computer, software, has, been, written, for, serial, computation, to, solve, a, problem, an, algorithm, is, constructed, and, implemented, as, a, serial, stream, of, instructions, these, instructions, are, executed, on, a, central, processing, unit, on, one, computer, only, one, instruction, may, execute, at, a, time, after, that, instruction, is, finished, the, next, one, is, executed, parallel, computing, on, the, other, hand, uses, multiple, processing, elements, simultaneously, to, solve, a, problem, this, is, accomplished, by, breaking, the, problem, into, independent, parts, so, that, each, processing, element, can, execute, its, part, of, the, algorithm, simultaneously, with, the, others, the, processing, elements, can, be, diverse, and, include, resources, such, as, a, single, computer, with, multiple, processors, several, networked, computers, specialized, hardware, or, any, combination, of, the, above, historically, parallel, computing, was, used, for, scientific, computing, and, the, simulation, of, scientific, problems, particularly, in, the, natural, and, engineering, sciences, such, as, meteorology, this, led, to, the, design, of, parallel, hardware, and, software, as, well, as, high, performance, computing, frequency, scaling, was, the, dominant, reason, for, improvements, in, computer, performance, from, the, mid, 1980s, until, 2004, the, runtime, of, a, program, is, equal, to, the, number, of, instructions, multiplied, by, the, average, time, per, instruction, maintaining, everything, else, constant, increasing, the, clock, frequency, decreases, the, average, time, it, takes, to, execute, an, instruction, an, increase, in, frequency, thus, decreases, runtime, for, all, compute, bound, programs, however, power, consumption, p, by, a, chip, is, given, by, the, equation, p, c, v, 2, f, where, c, is, the, capacitance, being, switched, per, clock, cycle, proportional, to, the, number, of, transistors, whose, inputs, change, v, is, voltage, and, f, is, the, processor, frequency, cycles, per, second, increases, in, frequency, increase, the, amount, of, power, used, in, a, processor, increasing, processor, power, consumption, led, ultimately, to, intel, s, may, 8, 2004, cancellation, of, its, tejas, and, jayhawk, processors, which, is, generally, cited, as, the, end, of, frequency, scaling, as, the, dominant, computer, architecture, paradigm, to, deal, with, the, problem, of, power, consumption, and, overheating, the, major, central, processing, unit, cpu, or, processor, manufacturers, started, to, produce, power, efficient, processors, with, multiple, cores, the, core, is, the, computing, unit, of, the, processor, and, in, multi, core, processors, each, core, is, independent, and, can, access, the, same, memory, concurrently, multi, core, processors, have, brought, parallel, computing, to, desktop, computers, thus, parallelization, of, serial, programs, has, become, a, mainstream, programming, task, in, 2012, quad, core, processors, became, standard, for, desktop, computers, while, servers, have, 10, core, processors, from, moore, s, law, it, can, be, predicted, that, the, number, of, cores, per, processor, will, double, every, 18, 24, months, this, could, mean, that, after, 2020, a, typical, processor, will, have, dozens, or, hundreds, of, cores, however, in, reality, the, standard, is, somewhere, in, the, region, of, 4, to, 16, cores, with, some, designs, having, a, mix, of, performance, and, efficiency, cores, such, as, arm, s, big, little, design, due, to, thermal, and, design, constraints, an, operating, system, can, ensure, that, different, tasks, and, user, programs, are, run, in, parallel, on, the, available, cores, however, for, a, serial, software, program, to, take, full, advantage, of, the, multi, core, architecture, the, programmer, needs, to, restructure, and, parallelize, the, code, a, speed, up, of, application, software, runtime, will, no, longer, be, achieved, through, frequency, scaling, instead, programmers, will, need, to, parallelize, their, software, code, to, take, advantage, of, the, increasing, computing, power, of, multicore, architectures, optimally, the, speedup, from, parallelization, would, be, linear, doubling, the, number, of, processing, elements, should, halve, the, runtime, and, doubling, it, a, second, time, should, again, halve, the, runtime, however, very, few, parallel, algorithms, achieve, optimal, speedup, most, of, them, have, a, near, linear, speedup, for, small, numbers, of, processing, elements, which, flattens, out, into, a, constant, value, for, large, numbers, of, processing, elements, the, potential, speedup, of, an, algorithm, on, a, parallel, computing, platform, is, given, by, amdahl, s, law, where, since, s, latency, 1, 1, p, it, shows, that, a, small, part, of, the, program, which, cannot, be, parallelized, will, limit, the, overall, speedup, available, from, parallelization, a, program, solving, a, large, mathematical, or, engineering, problem, will, typically, consist, of, several, parallelizable, parts, and, several, non, parallelizable, serial, parts, if, the, non, parallelizable, part, of, a, program, accounts, for, 10, of, the, runtime, p, 0, 9, we, can, get, no, more, than, a, 10, times, speedup, regardless, of, how, many, processors, are, added, this, puts, an, upper, limit, on, the, usefulness, of, adding, more, parallel, execution, units, when, a, task, cannot, be, partitioned, because, of, sequential, constraints, the, application, of, more, effort, has, no, effect, on, the, schedule, the, bearing, of, a, child, takes, nine, months, no, matter, how, many, women, are, assigned, amdahl, s, law, only, applies, to, cases, where, the, problem, size, is, fixed, in, practice, as, more, computing, resources, become, available, they, tend, to, get, used, on, larger, problems, larger, datasets, and, the, time, spent, in, the, parallelizable, part, often, grows, much, faster, than, the, inherently, serial, work, in, this, case, gustafson, s, law, gives, a, less, pessimistic, and, more, realistic, assessment, of, parallel, performance, both, amdahl, s, law, and, gustafson, s, law, assume, that, the, running, time, of, the, serial, part, of, the, program, is, independent, of, the, number, of, processors, amdahl, s, law, assumes, that, the, entire, problem, is, of, fixed, size, so, that, the, total, amount, of, work, to, be, done, in, parallel, is, also, independent, of, the, number, of, processors, whereas, gustafson, s, law, assumes, that, the, total, amount, of, work, to, be, done, in, parallel, varies, linearly, with, the, number, of, processors, understanding, data, dependencies, is, fundamental, in, implementing, parallel, algorithms, no, program, can, run, more, quickly, than, the, longest, chain, of, dependent, calculations, known, as, the, critical, path, since, calculations, that, depend, upon, prior, calculations, in, the, chain, must, be, executed, in, order, however, most, algorithms, do, not, consist, of, just, a, long, chain, of, dependent, calculations, there, are, usually, opportunities, to, execute, independent, calculations, in, parallel, let, p, i, and, p, j, be, two, program, segments, bernstein, s, conditions, describe, when, the, two, are, independent, and, can, be, executed, in, parallel, for, p, i, let, i, i, be, all, of, the, input, variables, and, o, i, the, output, variables, and, likewise, for, p, j, p, i, and, p, j, are, independent, if, they, satisfy, violation, of, the, first, condition, introduces, a, flow, dependency, corresponding, to, the, first, segment, producing, a, result, used, by, the, second, segment, the, second, condition, represents, an, anti, dependency, when, the, second, segment, produces, a, variable, needed, by, the, first, segment, the, third, and, final, condition, represents, an, output, dependency, when, two, segments, write, to, the, same, location, the, result, comes, from, the, logically, last, executed, segment, consider, the, following, functions, which, demonstrate, several, kinds, of, dependencies, in, this, example, instruction, 3, cannot, be, executed, before, or, even, in, parallel, with, instruction, 2, because, instruction, 3, uses, a, result, from, instruction, 2, it, violates, condition, 1, and, thus, introduces, a, flow, dependency, in, this, example, there, are, no, dependencies, between, the, instructions, so, they, can, all, be, run, in, parallel, bernstein, s, conditions, do, not, allow, memory, to, be, shared, between, different, processes, for, that, some, means, of, enforcing, an, ordering, between, accesses, is, necessary, such, as, semaphores, barriers, or, some, other, synchronization, method, subtasks, in, a, parallel, program, are, often, called, threads, some, parallel, computer, architectures, use, smaller, lightweight, versions, of, threads, known, as, fibers, while, others, use, bigger, versions, known, as, processes, however, threads, is, generally, accepted, as, a, generic, term, for, subtasks, threads, will, often, need, synchronized, access, to, an, object, or, other, resource, for, example, when, they, must, update, a, variable, that, is, shared, between, them, without, synchronization, the, instructions, between, the, two, threads, may, be, interleaved, in, any, order, for, example, consider, the, following, program, if, instruction, 1b, is, executed, between, 1a, and, 3a, or, if, instruction, 1a, is, executed, between, 1b, and, 3b, the, program, will, produce, incorrect, data, this, is, known, as, a, race, condition, the, programmer, must, use, a, lock, to, provide, mutual, exclusion, a, lock, is, a, programming, language, construct, that, allows, one, thread, to, take, control, of, a, variable, and, prevent, other, threads, from, reading, or, writing, it, until, that, variable, is, unlocked, the, thread, holding, the, lock, is, free, to, execute, its, critical, section, the, section, of, a, program, that, requires, exclusive, access, to, some, variable, and, to, unlock, the, data, when, it, is, finished, therefore, to, guarantee, correct, program, execution, the, above, program, can, be, rewritten, to, use, locks, one, thread, will, successfully, lock, variable, v, while, the, other, thread, will, be, locked, out, unable, to, proceed, until, v, is, unlocked, again, this, guarantees, correct, execution, of, the, program, locks, may, be, necessary, to, ensure, correct, program, execution, when, threads, must, serialize, access, to, resources, but, their, use, can, greatly, slow, a, program, and, may, affect, its, reliability, locking, multiple, variables, using, non, atomic, locks, introduces, the, possibility, of, program, deadlock, an, atomic, lock, locks, multiple, variables, all, at, once, if, it, cannot, lock, all, of, them, it, does, not, lock, any, of, them, if, two, threads, each, need, to, lock, the, same, two, variables, using, non, atomic, locks, it, is, possible, that, one, thread, will, lock, one, of, them, and, the, second, thread, will, lock, the, second, variable, in, such, a, case, neither, thread, can, complete, and, deadlock, results, many, parallel, programs, require, that, their, subtasks, act, in, synchrony, this, requires, the, use, of, a, barrier, barriers, are, typically, implemented, using, a, lock, or, a, semaphore, one, class, of, algorithms, known, as, lock, free, and, wait, free, algorithms, altogether, avoids, the, use, of, locks, and, barriers, however, this, approach, is, generally, difficult, to, implement, and, requires, correctly, designed, data, structures, not, all, parallelization, results, in, speed, up, generally, as, a, task, is, split, up, into, more, and, more, threads, those, threads, spend, an, ever, increasing, portion, of, their, time, communicating, with, each, other, or, waiting, on, each, other, for, access, to, resources, once, the, overhead, from, resource, contention, or, communication, dominates, the, time, spent, on, other, computation, further, parallelization, that, is, splitting, the, workload, over, even, more, threads, increases, rather, than, decreases, the, amount, of, time, required, to, finish, this, problem, known, as, parallel, slowdown, can, be, improved, in, some, cases, by, software, analysis, and, redesign, applications, are, often, classified, according, to, how, often, their, subtasks, need, to, synchronize, or, communicate, with, each, other, an, application, exhibits, fine, grained, parallelism, if, its, subtasks, must, communicate, many, times, per, second, it, exhibits, coarse, grained, parallelism, if, they, do, not, communicate, many, times, per, second, and, it, exhibits, embarrassing, parallelism, if, they, rarely, or, never, have, to, communicate, embarrassingly, parallel, applications, are, considered, the, easiest, to, parallelize, michael, j, flynn, created, one, of, the, earliest, classification, systems, for, parallel, and, sequential, computers, and, programs, now, known, as, flynn, s, taxonomy, flynn, classified, programs, and, computers, by, whether, they, were, operating, using, a, single, set, or, multiple, sets, of, instructions, and, whether, or, not, those, instructions, were, using, a, single, set, or, multiple, sets, of, data, the, single, instruction, single, data, sisd, classification, is, equivalent, to, an, entirely, sequential, program, the, single, instruction, multiple, data, simd, classification, is, analogous, to, doing, the, same, operation, repeatedly, over, a, large, data, set, this, is, commonly, done, in, signal, processing, applications, multiple, instruction, single, data, misd, is, a, rarely, used, classification, while, computer, architectures, to, deal, with, this, were, devised, such, as, systolic, arrays, few, applications, that, fit, this, class, materialized, multiple, instruction, multiple, data, mimd, programs, are, by, far, the, most, common, type, of, parallel, programs, according, to, david, a, patterson, and, john, l, hennessy, some, machines, are, hybrids, of, these, categories, of, course, but, this, classic, model, has, survived, because, it, is, simple, easy, to, understand, and, gives, a, good, first, approximation, it, is, also, perhaps, because, of, its, understandability, the, most, widely, used, scheme, from, the, advent, of, very, large, scale, integration, vlsi, computer, chip, fabrication, technology, in, the, 1970s, until, about, 1986, speed, up, in, computer, architecture, was, driven, by, doubling, computer, word, size, the, amount, of, information, the, processor, can, manipulate, per, cycle, increasing, the, word, size, reduces, the, number, of, instructions, the, processor, must, execute, to, perform, an, operation, on, variables, whose, sizes, are, greater, than, the, length, of, the, word, for, example, where, an, 8, bit, processor, must, add, two, 16, bit, integers, the, processor, must, first, add, the, 8, lower, order, bits, from, each, integer, using, the, standard, addition, instruction, then, add, the, 8, higher, order, bits, using, an, add, with, carry, instruction, and, the, carry, bit, from, the, lower, order, addition, thus, an, 8, bit, processor, requires, two, instructions, to, complete, a, single, operation, where, a, 16, bit, processor, would, be, able, to, complete, the, operation, with, a, single, instruction, historically, 4, bit, microprocessors, were, replaced, with, 8, bit, then, 16, bit, then, 32, bit, microprocessors, this, trend, generally, came, to, an, end, with, the, introduction, of, 32, bit, processors, which, has, been, a, standard, in, general, purpose, computing, for, two, decades, not, until, the, early, 2000s, with, the, advent, of, x86, 64, architectures, did, 64, bit, processors, become, commonplace, a, computer, program, is, in, essence, a, stream, of, instructions, executed, by, a, processor, without, instruction, level, parallelism, a, processor, can, only, issue, less, than, one, instruction, per, clock, cycle, ipc, 1, these, processors, are, known, as, subscalar, processors, these, instructions, can, be, re, ordered, and, combined, into, groups, which, are, then, executed, in, parallel, without, changing, the, result, of, the, program, this, is, known, as, instruction, level, parallelism, advances, in, instruction, level, parallelism, dominated, computer, architecture, from, the, mid, 1980s, until, the, mid, 1990s, all, modern, processors, have, multi, stage, instruction, pipelines, each, stage, in, the, pipeline, corresponds, to, a, different, action, the, processor, performs, on, that, instruction, in, that, stage, a, processor, with, an, n, stage, pipeline, can, have, up, to, n, different, instructions, at, different, stages, of, completion, and, thus, can, issue, one, instruction, per, clock, cycle, ipc, 1, these, processors, are, known, as, scalar, processors, the, canonical, example, of, a, pipelined, processor, is, a, risc, processor, with, five, stages, instruction, fetch, if, instruction, decode, id, execute, ex, memory, access, mem, and, register, write, back, wb, the, pentium, 4, processor, had, a, 35, stage, pipeline, most, modern, processors, also, have, multiple, execution, units, they, usually, combine, this, feature, with, pipelining, and, thus, can, issue, more, than, one, instruction, per, clock, cycle, ipc, 1, these, processors, are, known, as, superscalar, processors, superscalar, processors, differ, from, multi, core, processors, in, that, the, several, execution, units, are, not, entire, processors, i, e, processing, units, instructions, can, be, grouped, together, only, if, there, is, no, data, dependency, between, them, scoreboarding, and, the, tomasulo, algorithm, which, is, similar, to, scoreboarding, but, makes, use, of, register, renaming, are, two, of, the, most, common, techniques, for, implementing, out, of, order, execution, and, instruction, level, parallelism, task, parallelisms, is, the, characteristic, of, a, parallel, program, that, entirely, different, calculations, can, be, performed, on, either, the, same, or, different, sets, of, data, this, contrasts, with, data, parallelism, where, the, same, calculation, is, performed, on, the, same, or, different, sets, of, data, task, parallelism, involves, the, decomposition, of, a, task, into, sub, tasks, and, then, allocating, each, sub, task, to, a, processor, for, execution, the, processors, would, then, execute, these, sub, tasks, concurrently, and, often, cooperatively, task, parallelism, does, not, usually, scale, with, the, size, of, a, problem, superword, level, parallelism, is, a, vectorization, technique, based, on, loop, unrolling, and, basic, block, vectorization, it, is, distinct, from, loop, vectorization, algorithms, in, that, it, can, exploit, parallelism, of, inline, code, such, as, manipulating, coordinates, color, channels, or, in, loops, unrolled, by, hand, main, memory, in, a, parallel, computer, is, either, shared, memory, shared, between, all, processing, elements, in, a, single, address, space, or, distributed, memory, in, which, each, processing, element, has, its, own, local, address, space, distributed, memory, refers, to, the, fact, that, the, memory, is, logically, distributed, but, often, implies, that, it, is, physically, distributed, as, well, distributed, shared, memory, and, memory, virtualization, combine, the, two, approaches, where, the, processing, element, has, its, own, local, memory, and, access, to, the, memory, on, non, local, processors, accesses, to, local, memory, are, typically, faster, than, accesses, to, non, local, memory, on, the, supercomputers, distributed, shared, memory, space, can, be, implemented, using, the, programming, model, such, as, pgas, this, model, allows, processes, on, one, compute, node, to, transparently, access, the, remote, memory, of, another, compute, node, all, compute, nodes, are, also, connected, to, an, external, shared, memory, system, via, high, speed, interconnect, such, as, infiniband, this, external, shared, memory, system, is, known, as, burst, buffer, which, is, typically, built, from, arrays, of, non, volatile, memory, physically, distributed, across, multiple, i, o, nodes, computer, architectures, in, which, each, element, of, main, memory, can, be, accessed, with, equal, latency, and, bandwidth, are, known, as, uniform, memory, access, uma, systems, typically, that, can, be, achieved, only, by, a, shared, memory, system, in, which, the, memory, is, not, physically, distributed, a, system, that, does, not, have, this, property, is, known, as, a, non, uniform, memory, access, numa, architecture, distributed, memory, systems, have, non, uniform, memory, access, computer, systems, make, use, of, caches, small, and, fast, memories, located, close, to, the, processor, which, store, temporary, copies, of, memory, values, nearby, in, both, the, physical, and, logical, sense, parallel, computer, systems, have, difficulties, with, caches, that, may, store, the, same, value, in, more, than, one, location, with, the, possibility, of, incorrect, program, execution, these, computers, require, a, cache, coherency, system, which, keeps, track, of, cached, values, and, strategically, purges, them, thus, ensuring, correct, program, execution, bus, snooping, is, one, of, the, most, common, methods, for, keeping, track, of, which, values, are, being, accessed, and, thus, should, be, purged, designing, large, high, performance, cache, coherence, systems, is, a, very, difficult, problem, in, computer, architecture, as, a, result, shared, memory, computer, architectures, do, not, scale, as, well, as, distributed, memory, systems, do, processor, processor, and, processor, memory, communication, can, be, implemented, in, hardware, in, several, ways, including, via, shared, either, multiported, or, multiplexed, memory, a, crossbar, switch, a, shared, bus, or, an, interconnect, network, of, a, myriad, of, topologies, including, star, ring, tree, hypercube, fat, hypercube, a, hypercube, with, more, than, one, processor, at, a, node, or, n, dimensional, mesh, parallel, computers, based, on, interconnected, networks, need, to, have, some, kind, of, routing, to, enable, the, passing, of, messages, between, nodes, that, are, not, directly, connected, the, medium, used, for, communication, between, the, processors, is, likely, to, be, hierarchical, in, large, multiprocessor, machines, parallel, computers, can, be, roughly, classified, according, to, the, level, at, which, the, hardware, supports, parallelism, this, classification, is, broadly, analogous, to, the, distance, between, basic, computing, nodes, these, are, not, mutually, exclusive, for, example, clusters, of, symmetric, multiprocessors, are, relatively, common, a, multi, core, processor, is, a, processor, that, includes, multiple, processing, units, called, cores, on, the, same, chip, this, processor, differs, from, a, superscalar, processor, which, includes, multiple, execution, units, and, can, issue, multiple, instructions, per, clock, cycle, from, one, instruction, stream, thread, in, contrast, a, multi, core, processor, can, issue, multiple, instructions, per, clock, cycle, from, multiple, instruction, streams, ibm, s, cell, microprocessor, designed, for, use, in, the, sony, playstation, 3, is, a, prominent, multi, core, processor, each, core, in, a, multi, core, processor, can, potentially, be, superscalar, as, well, that, is, on, every, clock, cycle, each, core, can, issue, multiple, instructions, from, one, thread, simultaneous, multithreading, of, which, intel, s, hyper, threading, is, the, best, known, was, an, early, form, of, pseudo, multi, coreism, a, processor, capable, of, concurrent, multithreading, includes, multiple, execution, units, in, the, same, processing, unit, that, is, it, has, a, superscalar, architecture, and, can, issue, multiple, instructions, per, clock, cycle, from, multiple, threads, temporal, multithreading, on, the, other, hand, includes, a, single, execution, unit, in, the, same, processing, unit, and, can, issue, one, instruction, at, a, time, from, multiple, threads, a, symmetric, multiprocessor, smp, is, a, computer, system, with, multiple, identical, processors, that, share, memory, and, connect, via, a, bus, bus, contention, prevents, bus, architectures, from, scaling, as, a, result, smps, generally, do, not, comprise, more, than, 32, processors, because, of, the, small, size, of, the, processors, and, the, significant, reduction, in, the, requirements, for, bus, bandwidth, achieved, by, large, caches, such, symmetric, multiprocessors, are, extremely, cost, effective, provided, that, a, sufficient, amount, of, memory, bandwidth, exists, a, distributed, computer, also, known, as, a, distributed, memory, multiprocessor, is, a, distributed, memory, computer, system, in, which, the, processing, elements, are, connected, by, a, network, distributed, computers, are, highly, scalable, the, terms, concurrent, computing, parallel, computing, and, distributed, computing, have, a, lot, of, overlap, and, no, clear, distinction, exists, between, them, the, same, system, may, be, characterized, both, as, parallel, and, distributed, the, processors, in, a, typical, distributed, system, run, concurrently, in, parallel, a, cluster, is, a, group, of, loosely, coupled, computers, that, work, together, closely, so, that, in, some, respects, they, can, be, regarded, as, a, single, computer, clusters, are, composed, of, multiple, standalone, machines, connected, by, a, network, while, machines, in, a, cluster, do, not, have, to, be, symmetric, load, balancing, is, more, difficult, if, they, are, not, the, most, common, type, of, cluster, is, the, beowulf, cluster, which, is, a, cluster, implemented, on, multiple, identical, commercial, off, the, shelf, computers, connected, with, a, tcp, ip, ethernet, local, area, network, beowulf, technology, was, originally, developed, by, thomas, sterling, and, donald, becker, 87, of, all, top500, supercomputers, are, clusters, the, remaining, are, massively, parallel, processors, explained, below, because, grid, computing, systems, described, below, can, easily, handle, embarrassingly, parallel, problems, modern, clusters, are, typically, designed, to, handle, more, difficult, problems, problems, that, require, nodes, to, share, intermediate, results, with, each, other, more, often, this, requires, a, high, bandwidth, and, more, importantly, a, low, latency, interconnection, network, many, historic, and, current, supercomputers, use, customized, high, performance, network, hardware, specifically, designed, for, cluster, computing, such, as, the, cray, gemini, network, as, of, 2014, most, current, supercomputers, use, some, off, the, shelf, standard, network, hardware, often, myrinet, infiniband, or, gigabit, ethernet, a, massively, parallel, processor, mpp, is, a, single, computer, with, many, networked, processors, mpps, have, many, of, the, same, characteristics, as, clusters, but, mpps, have, specialized, interconnect, networks, whereas, clusters, use, commodity, hardware, for, networking, mpps, also, tend, to, be, larger, than, clusters, typically, having, far, more, than, 100, processors, in, an, mpp, each, cpu, contains, its, own, memory, and, copy, of, the, operating, system, and, application, each, subsystem, communicates, with, the, others, via, a, high, speed, interconnect, ibm, s, blue, gene, l, the, fifth, fastest, supercomputer, in, the, world, according, to, the, june, 2009, top500, ranking, is, an, mpp, grid, computing, is, the, most, distributed, form, of, parallel, computing, it, makes, use, of, computers, communicating, over, the, internet, to, work, on, a, given, problem, because, of, the, low, bandwidth, and, extremely, high, latency, available, on, the, internet, distributed, computing, typically, deals, only, with, embarrassingly, parallel, problems, most, grid, computing, applications, use, middleware, software, that, sits, between, the, operating, system, and, the, application, to, manage, network, resources, and, standardize, the, software, interface, the, most, common, grid, computing, middleware, is, the, berkeley, open, infrastructure, for, network, computing, boinc, often, volunteer, computing, software, makes, use, of, spare, cycles, performing, computations, at, times, when, a, computer, is, idling, the, ubiquity, of, internet, brought, the, possibility, of, large, scale, cloud, computing, within, parallel, computing, there, are, specialized, parallel, devices, that, remain, niche, areas, of, interest, while, not, domain, specific, they, tend, to, be, applicable, to, only, a, few, classes, of, parallel, problems, reconfigurable, computing, is, the, use, of, a, field, programmable, gate, array, fpga, as, a, co, processor, to, a, general, purpose, computer, an, fpga, is, in, essence, a, computer, chip, that, can, rewire, itself, for, a, given, task, fpgas, can, be, programmed, with, hardware, description, languages, such, as, vhdl, or, verilog, several, vendors, have, created, c, to, hdl, languages, that, attempt, to, emulate, the, syntax, and, semantics, of, the, c, programming, language, with, which, most, programmers, are, familiar, the, best, known, c, to, hdl, languages, are, mitrion, c, impulse, c, and, handel, c, specific, subsets, of, systemc, based, on, c, can, also, be, used, for, this, purpose, amd, s, decision, to, open, its, hypertransport, technology, to, third, party, vendors, has, become, the, enabling, technology, for, high, performance, reconfigurable, computing, according, to, michael, r, d, amour, chief, operating, officer, of, drc, computer, corporation, when, we, first, walked, into, amd, they, called, us, the, socket, stealers, now, they, call, us, their, partners, general, purpose, computing, on, graphics, processing, units, gpgpu, is, a, fairly, recent, trend, in, computer, engineering, research, gpus, are, co, processors, that, have, been, heavily, optimized, for, computer, graphics, processing, computer, graphics, processing, is, a, field, dominated, by, data, parallel, operations, particularly, linear, algebra, matrix, operations, in, the, early, days, gpgpu, programs, used, the, normal, graphics, apis, for, executing, programs, however, several, new, programming, languages, and, platforms, have, been, built, to, do, general, purpose, computation, on, gpus, with, both, nvidia, and, amd, releasing, programming, environments, with, cuda, and, stream, sdk, respectively, other, gpu, programming, languages, include, brookgpu, peakstream, and, rapidmind, nvidia, has, also, released, specific, products, for, computation, in, their, tesla, series, the, technology, consortium, khronos, group, has, released, the, opencl, specification, which, is, a, framework, for, writing, programs, that, execute, across, platforms, consisting, of, cpus, and, gpus, amd, apple, intel, nvidia, and, others, are, supporting, opencl, several, application, specific, integrated, circuit, asic, approaches, have, been, devised, for, dealing, with, parallel, applications, because, an, asic, is, by, definition, specific, to, a, given, application, it, can, be, fully, optimized, for, that, application, as, a, result, for, a, given, application, an, asic, tends, to, outperform, a, general, purpose, computer, however, asics, are, created, by, uv, photolithography, this, process, requires, a, mask, set, which, can, be, extremely, expensive, a, mask, set, can, cost, over, a, million, us, dollars, the, smaller, the, transistors, required, for, the, chip, the, more, expensive, the, mask, will, be, meanwhile, performance, increases, in, general, purpose, computing, over, time, as, described, by, moore, s, law, tend, to, wipe, out, these, gains, in, only, one, or, two, chip, generations, high, initial, cost, and, the, tendency, to, be, overtaken, by, moore, s, law, driven, general, purpose, computing, has, rendered, asics, unfeasible, for, most, parallel, computing, applications, however, some, have, been, built, one, example, is, the, pflops, riken, mdgrape, 3, machine, which, uses, custom, asics, for, molecular, dynamics, simulation, a, vector, processor, is, a, cpu, or, computer, system, that, can, execute, the, same, instruction, on, large, sets, of, data, vector, processors, have, high, level, operations, that, work, on, linear, arrays, of, numbers, or, vectors, an, example, vector, operation, is, a, b, c, where, a, b, and, c, are, each, 64, element, vectors, of, 64, bit, floating, point, numbers, they, are, closely, related, to, flynn, s, simd, classification, cray, computers, became, famous, for, their, vector, processing, computers, in, the, 1970s, and, 1980s, however, vector, processors, both, as, cpus, and, as, full, computer, systems, have, generally, disappeared, modern, processor, instruction, sets, do, include, some, vector, processing, instructions, such, as, with, freescale, semiconductor, s, altivec, and, intel, s, streaming, simd, extensions, sse, concurrent, programming, languages, libraries, apis, and, parallel, programming, models, such, as, algorithmic, skeletons, have, been, created, for, programming, parallel, computers, these, can, generally, be, divided, into, classes, based, on, the, assumptions, they, make, about, the, underlying, memory, architecture, shared, memory, distributed, memory, or, shared, distributed, memory, shared, memory, programming, languages, communicate, by, manipulating, shared, memory, variables, distributed, memory, uses, message, passing, posix, threads, and, openmp, are, two, of, the, most, widely, used, shared, memory, apis, whereas, message, passing, interface, mpi, is, the, most, widely, used, message, passing, system, api, one, concept, used, in, programming, parallel, programs, is, the, future, concept, where, one, part, of, a, program, promises, to, deliver, a, required, datum, to, another, part, of, a, program, at, some, future, time, efforts, to, standardize, parallel, programming, include, an, open, standard, called, openhmpp, for, hybrid, multi, core, parallel, programming, the, openhmpp, directive, based, programming, model, offers, a, syntax, to, efficiently, offload, computations, on, hardware, accelerators, and, to, optimize, data, movement, to, from, the, hardware, memory, using, remote, procedure, calls, the, rise, of, consumer, gpus, has, led, to, support, for, compute, kernels, either, in, graphics, apis, referred, to, as, compute, shaders, in, dedicated, apis, such, as, opencl, or, in, other, language, extensions, automatic, parallelization, of, a, sequential, program, by, a, compiler, is, the, holy, grail, of, parallel, computing, especially, with, the, aforementioned, limit, of, processor, frequency, despite, decades, of, work, by, compiler, researchers, automatic, parallelization, has, had, only, limited, success, mainstream, parallel, programming, languages, remain, either, explicitly, parallel, or, at, best, partially, implicit, in, which, a, programmer, gives, the, compiler, directives, for, parallelization, a, few, fully, implicit, parallel, programming, languages, exist, sisal, parallel, haskell, sequencel, system, c, for, fpgas, mitrion, c, vhdl, and, verilog, as, a, computer, system, grows, in, complexity, the, mean, time, between, failures, usually, decreases, application, checkpointing, is, a, technique, whereby, the, computer, system, takes, a, snapshot, of, the, application, a, record, of, all, current, resource, allocations, and, variable, states, akin, to, a, core, dump, this, information, can, be, used, to, restore, the, program, if, the, computer, should, fail, application, checkpointing, means, that, the, program, has, to, restart, from, only, its, last, checkpoint, rather, than, the, beginning, while, checkpointing, provides, benefits, in, a, variety, of, situations, it, is, especially, useful, in, highly, parallel, systems, with, a, large, number, of, processors, used, in, high, performance, computing, as, parallel, computers, become, larger, and, faster, we, are, now, able, to, solve, problems, that, had, previously, taken, too, long, to, run, fields, as, varied, as, bioinformatics, for, protein, folding, and, sequence, analysis, and, economics, have, taken, advantage, of, parallel, computing, common, types, of, problems, in, parallel, computing, applications, include, parallel, computing, can, also, be, applied, to, the, design, of, fault, tolerant, computer, systems, particularly, via, lockstep, systems, performing, the, same, operation, in, parallel, this, provides, redundancy, in, case, one, component, fails, and, also, allows, automatic, error, detection, and, error, correction, if, the, results, differ, these, methods, can, be, used, to, help, prevent, single, event, upsets, caused, by, transient, errors, although, additional, measures, may, be, required, in, embedded, or, specialized, systems, this, method, can, provide, a, cost, effective, approach, to, achieve, n, modular, redundancy, in, commercial, off, the, shelf, systems, the, origins, of, true, mimd, parallelism, go, back, to, luigi, federico, menabrea, and, his, sketch, of, the, analytic, engine, invented, by, charles, babbage, in, 1957, compagnie, des, machines, bull, announced, the, first, computer, architecture, specifically, designed, for, parallelism, the, gamma, 60, it, utilized, a, fork, join, model, and, a, program, distributor, to, dispatch, and, collect, data, to, and, from, independent, processing, units, connected, to, a, central, memory, in, april, 1958, stanley, gill, ferranti, discussed, parallel, programming, and, the, need, for, branching, and, waiting, also, in, 1958, ibm, researchers, john, cocke, and, daniel, slotnick, discussed, the, use, of, parallelism, in, numerical, calculations, for, the, first, time, burroughs, corporation, introduced, the, d825, in, 1962, a, four, processor, computer, that, accessed, up, to, 16, memory, modules, through, a, crossbar, switch, in, 1967, amdahl, and, slotnick, published, a, debate, about, the, feasibility, of, parallel, processing, at, american, federation, of, information, processing, societies, conference, it, was, during, this, debate, that, amdahl, s, law, was, coined, to, define, the, limit, of, speed, up, due, to, parallelism, in, 1969, honeywell, introduced, its, first, multics, system, a, symmetric, multiprocessor, system, capable, of, running, up, to, eight, processors, in, parallel, c, mmp, a, multi, processor, project, at, carnegie, mellon, university, in, the, 1970s, was, among, the, first, multiprocessors, with, more, than, a, few, processors, the, first, bus, connected, multiprocessor, with, snooping, caches, was, the, synapse, n, 1, in, 1984, simd, parallel, computers, can, be, traced, back, to, the, 1970s, the, motivation, behind, early, simd, computers, was, to, amortize, the, gate, delay, of, the, processor, s, control, unit, over, multiple, instructions, in, 1964, slotnick, had, proposed, building, a, massively, parallel, computer, for, the, lawrence, livermore, national, laboratory, his, design, was, funded, by, the, us, air, force, which, was, the, earliest, simd, parallel, computing, effort, illiac, iv, the, key, to, its, design, was, a, fairly, high, parallelism, with, up, to, 256, processors, which, allowed, the, machine, to, work, on, large, datasets, in, what, would, later, be, known, as, vector, processing, however, illiac, iv, was, called, the, most, infamous, of, supercomputers, because, the, project, was, only, one, fourth, completed, but, took, 11, years, and, cost, almost, four, times, the, original, estimate, when, it, was, finally, ready, to, run, its, first, real, application, in, 1976, it, was, outperformed, by, existing, commercial, supercomputers, such, as, the, cray, 1, in, the, early, 1970s, at, the, mit, computer, science, and, artificial, intelligence, laboratory, marvin, minsky, and, seymour, papert, started, developing, the, society, of, mind, theory, which, views, the, biological, brain, as, massively, parallel, computer, in, 1986, minsky, published, the, society, of, mind, which, claims, that, mind, is, formed, from, many, little, agents, each, mindless, by, itself, the, theory, attempts, to, explain, how, what, we, call, intelligence, could, be, a, product, of, the, interaction, of, non, intelligent, parts, minsky, says, that, the, biggest, source, of, ideas, about, the, theory, came, from, his, work, in, trying, to, create, a, machine, that, uses, a, robotic, arm, a, video, camera, and, a, computer, to, build, with, children, s, blocks, similar, models, which, also, view, the, biological, brain, as, a, massively, parallel, computer, i, e, the, brain, is, made, up, of, a, constellation, of, independent, or, semi, independent, agents, were, also, described, by